# Ansible Configuration Management with GitHub Actions

## Overview

In the previous lab, you used Terraform to provision a highly available multi-AZ infrastructure with an Auto Scaling Group and Application Load Balancer. The instances are running, but they're completely bare - just the operating system, nothing else.

Now you need to:
- Install Docker on all instances
- Deploy your FastAPI application container
- Ensure consistent configuration across the fleet
- Automate this process so it runs on every code push

In this lab, you'll use **Ansible** to configure your infrastructure and **GitHub Actions** to automate the deployment process using AWS Systems Manager (SSM) for secure, keyless access.

**What You'll Build:**
- Ansible playbook to install Docker and deploy containers
- AWS EC2 dynamic inventory to discover ASG instances
- GitHub Actions workflow with AWS OIDC authentication
- Automated configuration management pipeline

**No SSH keys required!** Everything uses AWS SSM Session Manager.

---

## Prerequisites

- Completed Lab 2.1 (Terraform Infrastructure) - **infrastructure must be running**
- Completed Lab 1.4 (GitHub OIDC Setup) - **OIDC provider and IAM role configured**
- Infrastructure still running from Lab 2.1 (VPC, ASG, ALB)
- IAM role with SSM permissions from Lab 1.4
- GitHub account
- Basic understanding of YAML and Ansible concepts

---

## Architecture Overview

```
GitHub Repository (Ansible)
        |
        | (push to main)
        v
GitHub Actions Workflow
        |
        | (assume IAM role via OIDC - from Lab 1.4)
        v
AWS IAM Role (SSM permissions)
        |
        v
AWS Systems Manager (SSM)
        |
        | (secure connection, no SSH keys)
        v
EC2 Instances in ASG (currently bare - just OS)
        |
        v
Ansible applies configuration:
  1. Update system packages
  2. Install Docker
  3. Start Docker service
  4. Pull FastAPI Docker image
  5. Run container on port 8000
  6. Verify application health
```

**Current State (from Lab 2.1):**
- VPC with subnets across 2 AZs âœ“
- Application Load Balancer âœ“
- Auto Scaling Group with EC2 instances âœ“
- SSM-enabled instances âœ“
- **But:** No Docker installed, no application running âŒ

**After This Lab:**
- Docker installed on all instances âœ“
- FastAPI application running in containers âœ“
- Automated deployment via GitHub Actions âœ“

---

## Part 1: Create Ansible Repository (5 minutes)

### 1.1 Create New Repository

1. Go to GitHub and create a new repository:
   - **Name:** `fastapi-ansible-deploy-{your-initials}`
   - **Description:** "Ansible deployment automation for FastAPI on AWS"
   - **Public** (required for OIDC to work easily)
   - Initialize with README

2. Clone it locally:
   ```bash
   git clone https://github.com/your-username/fastapi-ansible-deploy-{your-initials}.git
   cd fastapi-ansible-deploy-{your-initials}
   ```

### 1.2 Create Project Structure

```bash
mkdir -p playbooks roles/docker_app/{tasks,handlers,templates}
touch playbooks/deploy.yml
touch playbooks/aws_ec2.yml
touch ansible.cfg
touch roles/docker_app/tasks/main.yml
touch roles/docker_app/handlers/main.yml
touch .github/workflows/deploy.yml
touch README.md .gitignore
```

Your structure:
```
fastapi-ansible-deploy-{your-initials}/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ deploy.yml
â”‚   â””â”€â”€ aws_ec2.yml
â”œâ”€â”€ roles/
â”‚   â””â”€â”€ docker_app/
â”‚       â”œâ”€â”€ tasks/
â”‚       â”‚   â””â”€â”€ main.yml
â”‚       â””â”€â”€ handlers/
â”‚           â””â”€â”€ main.yml
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Part 2: Configure Ansible (15 minutes)

### 2.1 Create .gitignore

**`.gitignore`**:
```
# Ansible
*.retry
.ansible/
inventory.ini
*.log

# Python
__pycache__/
*.py[cod]
venv/
.venv/

# OS
.DS_Store
Thumbs.db

# Editor
.vscode/
.idea/
*.swp
```

### 2.2 Ansible Configuration

**`ansible.cfg`**:
```ini
[defaults]
# Use AWS EC2 dynamic inventory
inventory = ./playbooks/aws_ec2.yml

# Don't check host keys (instances are ephemeral)
host_key_checking = False

# Connection settings for SSM
stdout_callback = yaml
bin_ansible_callbacks = True

# Role path
roles_path = ./roles

# Gathering facts timeout
gather_timeout = 60

[inventory]
# Enable AWS EC2 plugin
enable_plugins = aws_ec2

[ssh_connection]
# SSM doesn't use traditional SSH
pipelining = False
```

### 2.3 AWS EC2 Dynamic Inventory

**`playbooks/aws_ec2.yml`**:
```yaml
---
plugin: aws_ec2

regions:
  - us-east-1  # Change if your infrastructure is in a different region

filters:
  # Only target instances with specific tags
  tag:Environment: dev
  instance-state-name: running

keyed_groups:
  # Group instances by environment tag
  - key: tags.Environment
    prefix: env
  
  # Group by instance type
  - key: instance_type
    prefix: type

hostnames:
  - instance-id  # Use instance ID as hostname

compose:
  # Use SSM for connection
  ansible_connection: aws_ssm
  ansible_aws_ssm_region: us-east-1
  ansible_aws_ssm_bucket_name: ""  # Empty means don't use S3 for session logs
```

### 2.4 Docker App Role - Tasks

**`roles/docker_app/tasks/main.yml`**:
```yaml
---
- name: Ensure Docker is installed
  ansible.builtin.dnf:
    name: docker
    state: present
  become: true

- name: Ensure Docker service is running
  ansible.builtin.systemd:
    name: docker
    state: started
    enabled: true
  become: true

- name: Add ec2-user to docker group
  ansible.builtin.user:
    name: ec2-user
    groups: docker
    append: true
  become: true

- name: Pull latest Docker image
  community.docker.docker_image:
    name: "{{ docker_image }}"
    source: pull
    force_source: true  # Always pull latest
  become: true
  notify: Restart container

- name: Ensure FastAPI container is running
  community.docker.docker_container:
    name: fastapi-app
    image: "{{ docker_image }}"
    state: started
    restart_policy: unless-stopped
    published_ports:
      - "80:8000"
    detach: true
  become: true
  register: container_result

- name: Wait for application to be ready
  ansible.builtin.uri:
    url: "http://localhost/health"
    status_code: 200
  retries: 10
  delay: 3
  register: health_check
  until: health_check.status == 200
```

### 2.5 Docker App Role - Handlers

**`roles/docker_app/handlers/main.yml`**:
```yaml
---
- name: Restart container
  community.docker.docker_container:
    name: fastapi-app
    state: started
    restart: true
  become: true
```

### 2.6 Main Playbook

**`playbooks/deploy.yml`**:
```yaml
---
- name: Deploy FastAPI Application
  hosts: env_dev  # Targets instances tagged with Environment: dev
  gather_facts: true
  become: false
  
  vars:
    docker_image: "your-dockerhub-username/fastapi-cicd:latest"  # UPDATE THIS!
  
  tasks:
    - name: Display target hosts
      ansible.builtin.debug:
        msg: "Deploying to {{ inventory_hostname }}"
    
    - name: Include docker_app role
      ansible.builtin.include_role:
        name: docker_app
```

---

## Part 3: Create Requirements File (2 minutes)

### 3.1 Python Requirements

Create `requirements.txt` in the root of your repository:

**`requirements.txt`**:
```
ansible>=8.0.0
boto3>=1.28.0
botocore>=1.31.0
```

These dependencies enable:
- **ansible:** Configuration management and orchestration
- **boto3/botocore:** AWS SDK for dynamic inventory and SSM connections

**Note:** The `cache: 'pip'` option in the GitHub Actions workflow will automatically cache these dependencies based on this file's hash, speeding up subsequent workflow runs.

---

## Part 4: Create GitHub Actions Workflow (15 minutes)

### 3.1 GitHub Actions Workflow

**`.github/workflows/deploy.yml`**:
```yaml
name: Deploy with Ansible

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allow manual triggers
    inputs:
      docker_image:
        description: 'Docker image to deploy'
        required: false
        default: 'latest'

permissions:
  id-token: write  # Required for OIDC
  contents: read

env:
  AWS_REGION: us-east-1

jobs:
  deploy:
    name: Deploy Application
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ansible boto3 botocore
          ansible-galaxy collection install amazon.aws
          ansible-galaxy collection install community.docker
      
      - name: Install AWS Session Manager Plugin
        run: |
          curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_64bit/session-manager-plugin.deb" -o "session-manager-plugin.deb"
          sudo dpkg -i session-manager-plugin.deb
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          role-session-name: GitHubActions-Ansible
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Test AWS credentials
        run: |
          aws sts get-caller-identity
          aws ec2 describe-instances --filters "Name=tag:Environment,Values=dev" --query 'Reservations[*].Instances[*].[InstanceId,State.Name]' --output table
      
      - name: Test Ansible inventory
        run: |
          ansible-inventory --list -i playbooks/aws_ec2.yml
      
      - name: Run Ansible playbook
        run: |
          ansible-playbook playbooks/deploy.yml -v
        env:
          ANSIBLE_HOST_KEY_CHECKING: false
      
      - name: Deployment summary
        run: |
          echo "âœ… Deployment completed successfully!"
          echo "Target environment: dev"
```

---

## Part 4: Configure GitHub Secret (2 minutes)

### 4.1 Add Secret to GitHub

1. Go to your Ansible repository on GitHub
2. Click **Settings** â†’ **Secrets and variables** â†’ **Actions**
3. Click **New repository secret**
4. Add:
   - **Name:** `AWS_ROLE_ARN`
   - **Value:** The role ARN from Lab 1.3 (e.g., `arn:aws:iam::123456789012:role/GitHubActionsAnsibleRole`)

---

## Part 5: Test the Pipeline (15 minutes)

### 6.1 Update the Playbook

Edit `playbooks/deploy.yml` and update the `docker_image` variable with your Docker Hub username:

```yaml
vars:
  docker_image: "yourusername/fastapi-cicd:latest"  # Change this to your Docker Hub username
```

### 6.2 Commit and Push

```bash
git add .
git commit -m "Add Ansible configuration and GitHub Actions workflow"
git push origin main
```

### 6.3 Monitor the Workflow

1. Go to your GitHub repository
2. Click **Actions** tab
3. You should see "Deploy with Ansible" workflow running
4. Click on it to see detailed logs

**Expected steps:**
1. âœ… Checkout code
2. âœ… Set up Python
3. âœ… Install Ansible and dependencies
4. âœ… Install AWS Session Manager Plugin
5. âœ… Configure AWS credentials (via OIDC)
6. âœ… Test AWS credentials
7. âœ… Test Ansible inventory (should find your instances)
8. âœ… Run Ansible playbook
9. âœ… Deployment summary

### 6.4 Verify Deployment

The workflow should show:
```
PLAY [Deploy FastAPI Application] ************

TASK [Display target hosts] ******************
ok: [i-0123456789abcdef0] => {
    "msg": "Deploying to i-0123456789abcdef0"
}
ok: [i-0123456789abcdef1] => {
    "msg": "Deploying to i-0123456789abcdef1"
}

TASK [Include docker_app role] ***************

TASK [docker_app : Ensure Docker is installed] ***
ok: [i-0123456789abcdef0]
ok: [i-0123456789abcdef1]

...

PLAY RECAP ***********************************
i-0123456789abcdef0    : ok=8    changed=0    unreachable=0    failed=0
i-0123456789abcdef1    : ok=8    changed=0    unreachable=0    failed=0
```

---

## Part 6: Test Application Updates (15 minutes)

### 7.1 Update FastAPI Application

Go back to your FastAPI application repository from Lab 1.0.

**Edit `app/main.py`:**
```python
app = FastAPI(
    title="Task Manager API",
    description="A simple task management API for CI/CD demonstration",
    version="2.0.0"  # Update version
)

@app.get("/")
def read_root():
    """Root endpoint returning API information"""
    return {
        "message": "Welcome to Task Manager API - Fully Automated!",  # Update message
        "version": "2.0.0",
        "deployment": "Automated with Terraform, Ansible, and GitHub Actions",
        "endpoints": ["/tasks", "/tasks/{task_id}", "/health"]
    }
```

### 7.2 Push Changes

```bash
git add app/main.py
git commit -m "Update to version 2.0.0 with automation message"
git push origin main
```

Wait for GitHub Actions CI/CD pipeline to build and push the new image (~2-3 minutes).

### 7.3 Trigger Ansible Deployment

**Option A: Automatic (if you add a webhook)**
- Set up a webhook to trigger Ansible workflow when Docker image is updated

**Option B: Manual trigger**
1. Go to your Ansible repository on GitHub
2. Click **Actions** â†’ **Deploy with Ansible**
3. Click **Run workflow** â†’ **Run workflow**

### 7.4 Watch the Deployment

The workflow will:
1. Connect to AWS via OIDC
2. Discover instances via dynamic inventory
3. Pull the latest Docker image (`2.0.0`)
4. Restart containers with new version
5. Verify health checks

### 7.5 Verify the Update

Get your ALB URL from Terraform:
```bash
cd ../fastapi-terraform-infra-{your-initials}/terraform
terraform output alb_url
```

Visit the URL in your browser. You should see:
```json
{
  "message": "Welcome to Task Manager API - Fully Automated!",
  "version": "2.0.0",
  "deployment": "Automated with Terraform, Ansible, and GitHub Actions",
  "endpoints": ["/tasks", "/tasks/{task_id}", "/health"]
}
```

**ðŸŽ‰ You just deployed an update across all instances without SSH!**

---

## Part 7: Test on Scaled Infrastructure (10 minutes)

### 8.1 Scale Up

Go back to your Terraform repository:
```bash
cd ../fastapi-terraform-infra-{your-initials}/terraform

# Edit terraform.tfvars
# Change: desired_capacity = 4

terraform apply
```

Wait for the new instances to launch and become healthy (~5 minutes).

### 8.2 Run Ansible Again

Trigger the Ansible workflow again (manually or via push).

Watch the logs - Ansible now discovers **4 instances** instead of 2 and configures all of them!

### 8.3 Verify All Instances

Check AWS Console:
1. **EC2** â†’ **Target Groups** â†’ Your target group
2. You should see 4 healthy targets
3. All running the latest container version

---

## Success Criteria

You have successfully completed this lab when:

- [ ] Ansible playbook is configured with SSM connection
- [ ] AWS EC2 dynamic inventory discovers instances by tags
- [ ] GitHub Actions workflow uses OIDC (no stored AWS credentials)
- [ ] Workflow successfully connects to AWS and runs Ansible
- [ ] Ansible manages Docker containers across all instances
- [ ] Application updates deploy automatically via the pipeline
- [ ] Scaling infrastructure up/down works with Ansible
- [ ] All deployments happen without SSH keys

---

## Understanding the Architecture

### What You Built

**Secure Access:**
- No SSH keys stored anywhere
- GitHub OIDC provides temporary credentials
- SSM Session Manager handles connections
- IAM roles enforce least-privilege access

**Dynamic Infrastructure:**
- Ansible automatically discovers instances via tags
- Works with Auto Scaling - no hardcoded IPs
- Scales from 2 to 200 instances seamlessly

**Automated Configuration:**
- Application updates via GitHub Actions
- Consistent configuration across all instances
- Idempotent - safe to run multiple times
- Auditability - all changes in Git history

**The Complete Pipeline:**
1. Developer pushes code to FastAPI repo
2. GitHub Actions builds and pushes Docker image
3. Developer triggers Ansible deployment (manual or webhook)
4. Ansible pulls latest image across all instances
5. Containers restart with new version
6. Health checks verify deployment success

---

## Cleanup

When finished with all labs:

```bash
# Destroy Terraform infrastructure
cd ../fastapi-terraform-infra-{your-initials}/terraform
terraform destroy

# IAM resources can be cleaned up as described in Lab 1.3
```

---

## Troubleshooting

**Ansible can't find instances:**
```bash
# Test dynamic inventory locally
ansible-inventory --list -i playbooks/aws_ec2.yml

# Verify AWS credentials
aws ec2 describe-instances --filters "Name=tag:Environment,Values=dev"

# Check region matches in aws_ec2.yml
```

**SSM connection fails:**
```bash
# Verify instances have SSM agent running
aws ssm describe-instance-information

# Check IAM role is attached to instances
# EC2 Console â†’ Instances â†’ Select instance â†’ Security â†’ IAM role

# Ensure Session Manager plugin is installed in GitHub Actions
```

**OIDC authentication fails:**
```bash
# Verify OIDC provider exists
aws iam list-open-id-connect-providers

# Check trust policy allows your repo
aws iam get-role --role-name GitHubActionsAnsibleRole

# Ensure repository is public (or configure for private)
```

**Docker container not updating:**
```bash
# Check if new image exists on Docker Hub
# Verify DOCKER_IMAGE secret is correct
# Check Ansible logs for pull errors
# Ensure handlers are triggering restart
```

---

## Key Takeaways

**What You Learned:**
- Configuration management with Ansible
- AWS EC2 dynamic inventory
- SSM for secure, keyless access
- GitHub OIDC for temporary credentials
- GitOps workflow for infrastructure
- Idempotent configuration management

**Why This Matters:**
- **No credential management:** OIDC eliminates long-lived secrets
- **Scalable:** Works with 2 or 2000 instances
- **Auditable:** Every deployment tracked in Git
- **Secure:** SSM Session Manager, no exposed SSH
- **Automated:** One push deploys everywhere
- **Separation of concerns:** Terraform for infrastructure, Ansible for configuration

**You've built a production-grade deployment pipeline!**

---

## Bonus Challenges

Want to level up? Try these:

1. **Add staging environment:**
   - Deploy to `Environment: staging` tag first
   - Manual approval before production
   - Different Docker image tags per environment

2. **Add monitoring:**
   - Deploy CloudWatch agent via Ansible
   - Collect container metrics
   - Set up alarms

3. **Blue/Green deployments:**
   - Use Ansible to deploy to half the instances
   - Verify health
   - Deploy to remaining instances

4. **Configuration templates:**
   - Use Jinja2 templates in Ansible
   - Deploy environment-specific configurations
   - Manage application config files

5. **Automated rollback:**
   - Tag Docker images with versions
   - If health checks fail, roll back to previous version

---

## Additional Resources

- [Ansible AWS Guide](https://docs.ansible.com/ansible/latest/collections/amazon/aws/)
- [GitHub Actions OIDC](https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services)
- [AWS Session Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html)
- [Ansible Dynamic Inventory](https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html)

---

**Congratulations!** You've mastered the complete DevOps automation pipeline: CI/CD â†’ Infrastructure as Code â†’ Configuration Management! ðŸš€
