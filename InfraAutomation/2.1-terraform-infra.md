# Lab 3.1: Terraform Multi-AZ Infrastructure Automation

## Overview

In the previous lab, you manually deployed an application to a single EC2 instance. That works, but what happens when you need high availability, auto-scaling, or consistent deployments across environments?

In this lab, you'll use **Terraform** to provision a production-ready, highly available infrastructure across multiple availability zones.

**What You'll Build:**
- VPC with public subnets across 2 AZs
- Application Load Balancer (ALB) to distribute traffic
- Auto Scaling Group (ASG) for high availability
- Launch Template for consistent instance configuration
- SSM-enabled instances for secure management (no SSH keys!)
- Security groups with proper access control

**Note:** This lab only provisions the infrastructure. In Lab 1.3, you'll use Ansible to install Docker and deploy your application.

---

## Prerequisites

- Completed Lab 1.0 (CI/CD Pipeline) and 1.1 (Manual Deployment)
- Docker image published to Docker Hub
- AWS credentials configured locally
- Terraform installed (v1.5+)
- Basic understanding of networking concepts

---

## Architecture Overview

```
                          Internet
                             |
                    [Application Load Balancer]
                       /              \
                      /                \
            [AZ us-east-1a]      [AZ us-east-1b]
            Public Subnet         Public Subnet
                  |                     |
            [Auto Scaling Group]        |
                  |_____________________|
                  |                     |
            [EC2 Instance]        [EC2 Instance]
            (SSM-enabled)         (SSM-enabled)
            
Instances are ready for Ansible configuration in the next lab
```

---

## Part 1: Create Terraform Repository (5 minutes)

### 1.1 Create New Repository

1. Go to GitHub and create a new repository:
   - **Name:** `fastapi-terraform-infra-{your-initials}`
   - **Description:** "Terraform IaC for multi-AZ FastAPI deployment"
   - **Public** or **Private** (your choice)
   - Initialize with README

2. Clone it locally:
   ```bash
   git clone https://github.com/your-username/fastapi-terraform-infra-{your-initials}.git
   cd fastapi-terraform-infra-{your-initials}
   ```

### 1.2 Create Project Structure

```bash
mkdir -p terraform/{modules/networking,modules/compute}
touch terraform/{main.tf,variables.tf,outputs.tf,terraform.tfvars}
touch terraform/modules/networking/{main.tf,variables.tf,outputs.tf}
touch terraform/modules/compute/{main.tf,variables.tf,outputs.tf}
touch README.md .gitignore
```

Your structure:
```
fastapi-terraform-infra-{your-initials}/
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfvars
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îÇ       ‚îú‚îÄ‚îÄ networking/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ       ‚îî‚îÄ‚îÄ compute/
‚îÇ           ‚îú‚îÄ‚îÄ main.tf
‚îÇ           ‚îú‚îÄ‚îÄ variables.tf
‚îÇ           ‚îî‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## Part 2: Configure Networking Module (10 minutes)

### 2.1 Create .gitignore

**`.gitignore`**:
```
# Terraform
*.tfstate
*.tfstate.*
.terraform/
.terraform.lock.hcl
terraform.tfvars
*.tfvars

# OS files
.DS_Store
Thumbs.db

# Editor files
*.swp
*.swo
*~
.vscode/
.idea/
```

### 2.2 Networking Module - Variables

**`terraform/modules/networking/variables.tf`**:
```hcl
variable "project_name" {
  description = "Project name for resource naming"
  type        = string
}

variable "environment" {
  description = "Environment name (dev, staging, prod)"
  type        = string
}

variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "availability_zones" {
  description = "List of availability zones"
  type        = list(string)
}

variable "public_subnet_cidrs" {
  description = "CIDR blocks for public subnets"
  type        = list(string)
}

variable "tags" {
  description = "Common tags for all resources"
  type        = map(string)
  default     = {}
}
```

### 2.3 Networking Module - Main Configuration

**`terraform/modules/networking/main.tf`**:
```hcl
# VPC
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-vpc"
    }
  )
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-igw"
    }
  )
}

# Public Subnets
resource "aws_subnet" "public" {
  count                   = length(var.public_subnet_cidrs)
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.public_subnet_cidrs[count.index]
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-public-subnet-${count.index + 1}"
      Tier = "Public"
    }
  )
}

# Public Route Table
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-public-rt"
    }
  )
}

# Public Route Table Associations
resource "aws_route_table_association" "public" {
  count          = length(aws_subnet.public)
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}
```

### 2.4 Networking Module - Outputs

**`terraform/modules/networking/outputs.tf`**:
```hcl
output "vpc_id" {
  description = "VPC ID"
  value       = aws_vpc.main.id
}

output "public_subnet_ids" {
  description = "List of public subnet IDs"
  value       = aws_subnet.public[*].id
}

output "vpc_cidr" {
  description = "VPC CIDR block"
  value       = aws_vpc.main.cidr_block
}
```

---

## Part 3: Configure Compute Module (15 minutes)

### 3.1 Compute Module - Variables

**`terraform/modules/compute/variables.tf`**:
```hcl
variable "project_name" {
  description = "Project name for resource naming"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "subnet_ids" {
  description = "List of subnet IDs for ASG"
  type        = list(string)
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t2.micro"
}

variable "desired_capacity" {
  description = "Desired number of instances"
  type        = number
  default     = 2
}

variable "min_size" {
  description = "Minimum number of instances"
  type        = number
  default     = 2
}

variable "max_size" {
  description = "Maximum number of instances"
  type        = number
  default     = 4
}

variable "health_check_grace_period" {
  description = "Time after instance launch before health checks start"
  type        = number
  default     = 300
}

variable "tags" {
  description = "Common tags for all resources"
  type        = map(string)
  default     = {}
}
```

### 3.2 Compute Module - Main Configuration

**`terraform/modules/compute/main.tf`**:
```hcl
# Data source for latest Amazon Linux 2023 AMI
data "aws_ami" "amazon_linux_2023" {
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["al2023-ami-*-x86_64"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

# IAM Role for EC2 instances (SSM access)
resource "aws_iam_role" "ec2_ssm_role" {
  name = "${var.project_name}-${var.environment}-ec2-ssm-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })

  tags = var.tags
}

# Attach AWS managed SSM policy to role
resource "aws_iam_role_policy_attachment" "ssm_policy" {
  role       = aws_iam_role.ec2_ssm_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

# Instance profile for EC2
resource "aws_iam_instance_profile" "ec2_profile" {
  name = "${var.project_name}-${var.environment}-ec2-profile"
  role = aws_iam_role.ec2_ssm_role.name

  tags = var.tags
}

# Security Group for ALB
resource "aws_security_group" "alb" {
  name        = "${var.project_name}-${var.environment}-alb-sg"
  description = "Security group for Application Load Balancer"
  vpc_id      = var.vpc_id

  ingress {
    description = "HTTP from internet"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    description = "Allow all outbound"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-alb-sg"
    }
  )
}

# Security Group for EC2 instances
resource "aws_security_group" "ec2" {
  name        = "${var.project_name}-${var.environment}-ec2-sg"
  description = "Security group for EC2 instances"
  vpc_id      = var.vpc_id

  ingress {
    description     = "HTTP from ALB"
    from_port       = 80
    to_port         = 80
    protocol        = "tcp"
    security_groups = [aws_security_group.alb.id]
  }

  egress {
    description = "Allow all outbound"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-ec2-sg"
    }
  )
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "${var.project_name}-${var.environment}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = var.subnet_ids

  enable_deletion_protection = false

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-alb"
    }
  )
}

# Target Group
resource "aws_lb_target_group" "main" {
  name     = "${var.project_name}-${var.environment}-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = var.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    unhealthy_threshold = 2
    timeout             = 5
    interval            = 30
    path                = "/health"
    matcher             = "200"
  }

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-${var.environment}-tg"
    }
  )
}

# ALB Listener
resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.main.arn
  port              = 80
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.main.arn
  }

  tags = var.tags
}

# Launch Template
resource "aws_launch_template" "main" {
  name_prefix   = "${var.project_name}-${var.environment}-"
  image_id      = data.aws_ami.amazon_linux_2023.id
  instance_type = var.instance_type

  iam_instance_profile {
    name = aws_iam_instance_profile.ec2_profile.name
  }

  vpc_security_group_ids = [aws_security_group.ec2.id]

  # User data to install Docker and run demo application
  user_data = base64encode(<<-EOF
    #!/bin/bash
    # Update system packages
    yum update -y
    
    # Install Docker
    yum install -y docker
    
    # Start and enable Docker service
    systemctl start docker
    systemctl enable docker
    
    # Add ec2-user to docker group (allows running Docker without sudo)
    usermod -a -G docker ec2-user
    
    # Install Python3 (required for Ansible)
    yum install -y python3
    
    # Run a simple web server as a demo (port 80)
    # This allows students to verify the infrastructure works immediately
    # TIP: Replace httpd:latest with your own Docker image from Lab 1.0!
    # Example: your-dockerhub-username/fastapi-cicd:latest
    docker run -d \
      --name demo-web \
      --restart unless-stopped \
      -p 80:80 \
      httpd:latest
    
    # Log completion
    echo "User data script completed at $(date)" >> /var/log/user-data.log
  EOF
  )

  metadata_options {
    http_endpoint               = "enabled"
    http_tokens                 = "required"
    http_put_response_hop_limit = 1
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = "instance"
    tags = merge(
      var.tags,
      {
        Name        = "${var.project_name}-${var.environment}-instance"
        Environment = var.environment
        ManagedBy   = "Terraform"
      }
    )
  }

  tags = var.tags
}

# Auto Scaling Group
resource "aws_autoscaling_group" "main" {
  name                = "${var.project_name}-${var.environment}-asg"
  vpc_zone_identifier = var.subnet_ids
  target_group_arns   = [aws_lb_target_group.main.arn]
  health_check_type   = "ELB"
  health_check_grace_period = var.health_check_grace_period

  desired_capacity = var.desired_capacity
  min_size         = var.min_size
  max_size         = var.max_size

  launch_template {
    id      = aws_launch_template.main.id
    version = "$Latest"
  }

  tag {
    key                 = "Name"
    value               = "${var.project_name}-${var.environment}-asg-instance"
    propagate_at_launch = true
  }

  tag {
    key                 = "Environment"
    value               = var.environment
    propagate_at_launch = true
  }

  tag {
    key                 = "ManagedBy"
    value               = "Terraform"
    propagate_at_launch = true
  }

  lifecycle {
    create_before_destroy = true
  }
}
```

### 3.3 Compute Module - Outputs

**`terraform/modules/compute/outputs.tf`**:
```hcl
output "alb_dns_name" {
  description = "DNS name of the Application Load Balancer"
  value       = aws_lb.main.dns_name
}

output "alb_arn" {
  description = "ARN of the Application Load Balancer"
  value       = aws_lb.main.arn
}

output "asg_name" {
  description = "Name of the Auto Scaling Group"
  value       = aws_autoscaling_group.main.name
}

output "target_group_arn" {
  description = "ARN of the target group"
  value       = aws_lb_target_group.main.arn
}

output "ec2_security_group_id" {
  description = "Security group ID for EC2 instances"
  value       = aws_security_group.ec2.id
}
```

---

## Part 4: Configure Root Terraform Module (10 minutes)

### 4.1 Root Module - Variables

**`terraform/variables.tf`**:
```hcl
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "project_name" {
  description = "Project name for resource naming"
  type        = string
}

variable "environment" {
  description = "Environment name (dev, staging, prod)"
  type        = string
  default     = "dev"
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t2.micro"
}

variable "desired_capacity" {
  description = "Desired number of instances in ASG"
  type        = number
  default     = 2
}

variable "min_size" {
  description = "Minimum number of instances in ASG"
  type        = number
  default     = 2
}

variable "max_size" {
  description = "Maximum number of instances in ASG"
  type        = number
  default     = 4
}
```

### 4.2 Root Module - Main Configuration

**`terraform/main.tf`**:
```hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region

  default_tags {
    tags = {
      Project     = var.project_name
      Environment = var.environment
      ManagedBy   = "Terraform"
    }
  }
}

# Local values
locals {
  availability_zones   = ["${var.aws_region}a", "${var.aws_region}b"]
  public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24"]
  
  common_tags = {
    Project     = var.project_name
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# Networking Module
module "networking" {
  source = "./modules/networking"

  project_name         = var.project_name
  environment          = var.environment
  vpc_cidr             = "10.0.0.0/16"
  availability_zones   = local.availability_zones
  public_subnet_cidrs  = local.public_subnet_cidrs
  tags                 = local.common_tags
}

# Compute Module
module "compute" {
  source = "./modules/compute"

  project_name              = var.project_name
  environment               = var.environment
  vpc_id                    = module.networking.vpc_id
  subnet_ids                = module.networking.public_subnet_ids
  instance_type             = var.instance_type
  desired_capacity          = var.desired_capacity
  min_size                  = var.min_size
  max_size                  = var.max_size
  health_check_grace_period = 300
  tags                      = local.common_tags
}
```

### 4.3 Root Module - Outputs

**`terraform/outputs.tf`**:
```hcl
output "alb_dns_name" {
  description = "DNS name of the Application Load Balancer - use this to access your application"
  value       = module.compute.alb_dns_name
}

output "alb_url" {
  description = "Full URL to access your application"
  value       = "http://${module.compute.alb_dns_name}"
}

output "vpc_id" {
  description = "VPC ID"
  value       = module.networking.vpc_id
}

output "asg_name" {
  description = "Auto Scaling Group name"
  value       = module.compute.asg_name
}

output "region" {
  description = "AWS region"
  value       = var.aws_region
}
```

### 4.4 Create terraform.tfvars

**`terraform/terraform.tfvars.example`** (commit this):
```hcl
# Copy this file to terraform.tfvars and fill in your values

project_name     = "fastapi"
environment      = "dev"
instance_type    = "t2.micro"
desired_capacity = 2
min_size         = 2
max_size         = 4
```

Create your actual `terraform.tfvars`:
```bash
cp terraform/terraform.tfvars.example terraform/terraform.tfvars
```

---

## Part 5: Deploy Infrastructure (10 minutes)

### 5.1 (Optional) Customize Docker Image

**Want to see your own application instead of Apache?**

Edit `terraform/modules/compute/main.tf` and find the user data section:

```hcl
docker run -d \
  --name demo-web \
  --restart unless-stopped \
  -p 80:80 \
  httpd:latest
```

**Replace `httpd:latest` with your Docker image from Lab 1.0:**
```hcl
docker run -d \
  --name demo-web \
  --restart unless-stopped \
  -p 80:8000 \  # If your app runs on 8000
  your-dockerhub-username/fastapi-cicd:latest
```

**Important:** If your app runs on a different port (like 8000), you need to map it:
- `-p 80:8000` means host port 80 ‚Üí container port 8000
- The ALB always sends traffic to port 80 on the host

**Note:** If you skip this, that's fine! The lab works with the default Apache image.

### 5.2 Initialize Terraform

```bash
cd terraform

# Initialize Terraform (download providers)
terraform init
```

### 5.3 Validate Configuration

```bash
# Check syntax
terraform validate

# Format code
terraform fmt -recursive
```

### 5.4 Plan Deployment

```bash
# See what will be created
# Note: terraform.tfvars is loaded automatically
terraform plan
```

Review the plan. You should see:
- 1 VPC
- 1 Internet Gateway
- 2 Public Subnets
- 1 Route Table
- 1 Application Load Balancer
- 1 Target Group
- 1 Launch Template
- 1 Auto Scaling Group
- 2 Security Groups
- IAM Role and Instance Profile

### 5.5 Apply Configuration

```bash
# Deploy infrastructure
# Note: terraform.tfvars is automatically loaded
terraform apply

# Type 'yes' when prompted
```

**Note:** Terraform automatically loads `terraform.tfvars` - you don't need to specify `-var-file`. If you named it differently (like `dev.tfvars`), you'd use: `terraform apply -var-file=dev.tfvars`

This will take 3-5 minutes. Terraform will create all resources and output the ALB DNS name.

### 5.6 Save Outputs

```bash
# View outputs
terraform output

# Save ALB URL
terraform output alb_url
```

---

## Part 6: Verify Infrastructure (10 minutes)

### 6.1 Check Instances

Check that instances launched:
1. Go to **EC2** ‚Üí **Instances**
2. You should see 2 instances with tag `Environment: dev`
3. Both should be in **running** state

### 6.2 Verify SSM Access

Check that instances are SSM-enabled:

1. Go to **AWS Systems Manager** ‚Üí **Fleet Manager**
2. You should see your instances listed
3. Click on an instance ‚Üí **Node actions** ‚Üí **Start terminal session**
4. You can connect without SSH keys!

Try connecting to verify SSM works. You'll use this in the next lab.

### 6.3 Verify Load Balancer

```bash
# Get the ALB URL
terraform output alb_url
```

Visit the URL in your browser. You should see:

**"It works!"** - The default Apache welcome page

This proves:
- ‚úÖ Load balancer is working
- ‚úÖ Instances are healthy and passing health checks
- ‚úÖ Docker is installed and running containers
- ‚úÖ Traffic is reaching the instances

**Note:** Ansible will replace this demo Apache container with your FastAPI application in Lab 2.2.
- ‚úÖ Docker is installed and running

**Note:** Ansible will replace this demo Apache container with your FastAPI application in Lab 2.2.

---

## Part 7: Test Auto Scaling (5 minutes)

### 7.1 Verify Current State

```bash
# Get ASG name
terraform output asg_name

# Check current instances in AWS Console
# EC2 ‚Üí Auto Scaling Groups ‚Üí Select your ASG ‚Üí Instance management
```

You should see 2 instances running.

### 7.2 Test Instance Replacement

Let's simulate an instance failure:

1. In AWS Console, go to **EC2** ‚Üí **Instances**
2. Find one of your ASG instances (has tag `Environment: dev`)
3. Select it ‚Üí **Instance state** ‚Üí **Terminate instance**

Watch what happens:
- ASG detects instance termination
- Automatically launches a new instance to maintain desired count
- New instance registers with SSM
- Registers with target group

**This is self-healing infrastructure!**

### 7.3 Test Manual Scaling

```bash
# Edit terraform.tfvars
# Change: desired_capacity = 3

# Apply the change
terraform apply

# Type 'yes'
```

Watch the ASG launch a third instance. Check the instances - you'll now have 3 running!

Scale back down:
```bash
# Edit terraform.tfvars
# Change: desired_capacity = 2

terraform apply
```

---

## Part 8: Understanding What You Built

**High Availability:**
- Instances spread across 2 availability zones
- Load balancer ready to distribute traffic
- Self-healing: ASG replaces failed instances

**Security:**
- Security group allows only ALB ‚Üí instances traffic on port 80
- SSM access (no SSH keys needed)
- IAM roles with least-privilege

**Infrastructure as Code:**
- Repeatable and version-controlled
- Modular design (networking + compute)
- Can deploy identical environments

**Next Step:**
In Lab 1.3, you'll use Ansible to install Docker and deploy your application across these instances using GitHub Actions and SSM.

---

## Success Criteria

You have successfully completed this lab when:

- [ ] Terraform code is organized in modules (networking + compute)
- [ ] Infrastructure deploys successfully with `terraform apply`
- [ ] 2 instances are running across 2 availability zones
- [ ] Instances have SSM access enabled
- [ ] Application Load Balancer and target group created
- [ ] Auto Scaling Group configured
- [ ] Can scale up/down by modifying `terraform.tfvars`
- [ ] ASG replaces terminated instances automatically

---

## Part 8: Cleanup (5 minutes)

### 8.1 Understanding the Problem

Before moving to the next lab, let's understand a critical issue with this approach:

**What happens when the Auto Scaling Group replaces an instance?**

1. ASG terminates an instance (health check failure, scaling down, etc.)
2. ASG launches a new instance from the launch template
3. User data runs ‚Üí Installs Docker, pulls `httpd:latest` (or your image)
4. If you manually configured something on the old instance... **it's gone!** ‚ùå

**The Problem:**
- User data only runs on **first boot**
- Perfect for system dependencies (Docker, Python, etc.)
- **NOT ideal** for applications that need updates
- Every new instance starts from scratch

**In the next lab**, you'll learn how to solve this with **Packer** and **immutable infrastructure** - baking your application directly into a custom AMI.

### 8.2 Destroy Infrastructure

**You must destroy this infrastructure before Lab 2.2!**

We'll rebuild with a better approach (custom AMI instead of user data).

```bash
# Destroy all resources
terraform destroy

# Type 'yes' when prompted
```

This will delete:
- All EC2 instances
- Auto Scaling Group
- Application Load Balancer
- VPC and networking
- IAM roles (for EC2 - OIDC role stays)

**Cost:** $0 after destruction ‚úÖ

### 8.3 Verify Cleanup

```bash
# Confirm everything is deleted
aws ec2 describe-instances --filters "Name=tag:Project,Values=fastapi" --query 'Reservations[*].Instances[*].[InstanceId,State.Name]' --output table

# Should show no running instances or "terminated" state
```

---

## Commit Your Code

```bash
git add .
git commit -m "Add Terraform infrastructure for multi-AZ deployment"
git push origin main
```

---

## Troubleshooting

**Terraform errors:**
```bash
# Re-initialize if providers changed
terraform init -upgrade

# Check AWS credentials
aws sts get-caller-identity

# Validate syntax
terraform validate
```

**Instances not appearing in SSM:**
- Wait 2-3 minutes after launch
- Check IAM role is attached to instances
- Verify instances are in running state

---

## Key Takeaways

**You built:**
- Multi-AZ VPC with Terraform modules
- Auto Scaling Group for high availability
- Application Load Balancer for traffic distribution
- SSM-enabled instances (no SSH keys!)

**Why it matters:**
- Infrastructure as Code: repeatable, version-controlled
- Modular design: reusable components
- Self-healing: ASG replaces failed instances
- Secure: SSM access, proper IAM roles

**Next Lab:** Use Ansible + GitHub Actions to install Docker and deploy your application across these instances!

---

## Additional Resources

- [Terraform AWS Provider Documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [AWS Auto Scaling Documentation](https://docs.aws.amazon.com/autoscaling/)
- [AWS Application Load Balancer](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/)
- [AWS Systems Manager Session Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html)

---

**Congratulations!** You've automated infrastructure deployment with Terraform. No more manual clicking in AWS Console! üéâ
