# Lab 5.1: Hybrid Cloud Disaster Recovery with Multi-Region Redundancy

## Overview

In this lab, you'll build a complete disaster recovery (DR) solution that simulates an on-premises data center with automated cloud backups and multi-region redundancy. You'll use Terraform to provision infrastructure, Ansible to configure servers, and AWS S3 cross-region replication to ensure data survives catastrophic failures.

**Real-world scenario:** Companies need to protect critical business data against both server failures and regional disasters. You'll implement a hybrid cloud backup strategy where an "on-premises" server continuously syncs data to AWS S3, with automatic replication to a secondary geographic region.

### What You'll Learn

- Infrastructure as Code with Terraform (multi-region architecture)
- Configuration Management with Ansible (automated backup setup)
- AWS S3 cross-region replication for geo-redundancy
- Disaster recovery procedures (RTO/RPO planning)
- Simulating and recovering from catastrophic failures
- Hybrid cloud backup strategies

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PRIMARY REGION (us-east-1)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  EC2 "On-Prem"   â”‚  Sync    â”‚   S3 Primary Bucket     â”‚     â”‚
â”‚  â”‚  Data Center     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   (Business Data)       â”‚     â”‚
â”‚  â”‚  Server          â”‚          â”‚                         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                            â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â”‚ Cross-Region
                                             â”‚ Replication
                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SECONDARY REGION (us-west-2)               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                    â”‚   S3 Replica Bucket     â”‚                  â”‚
â”‚                    â”‚   (DR Copy)             â”‚                  â”‚
â”‚                    â”‚                         â”‚                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Time Estimate
90-120 minutes

### Prerequisites

- AWS CLI configured with credentials
- Terraform installed (v1.0+)
- Ansible installed (v2.9+)
- SSH key pair for EC2 access
- Basic understanding of DR concepts

---

## Part 1: Provision Multi-Region Infrastructure with Terraform

Create the cloud infrastructure that will support your hybrid backup solution.

### Step 1: Create Terraform Configuration Directory

```bash
mkdir ~/dr-lab
cd ~/dr-lab
```

### Step 2: Create `main.tf` - Primary Infrastructure

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Primary region provider (us-east-1)
provider "aws" {
  region = var.primary_region
  alias  = "primary"
}

# Secondary region provider (us-west-2)
provider "aws" {
  region = var.secondary_region
  alias  = "secondary"
}

# VPC for "on-prem" server
resource "aws_vpc" "onprem" {
  provider             = aws.primary
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = "dr-lab-onprem-vpc"
  }
}

# Internet Gateway
resource "aws_internet_gateway" "onprem" {
  provider = aws.primary
  vpc_id   = aws_vpc.onprem.id

  tags = {
    Name = "dr-lab-igw"
  }
}

# Public Subnet
resource "aws_subnet" "public" {
  provider                = aws.primary
  vpc_id                  = aws_vpc.onprem.id
  cidr_block              = "10.0.1.0/24"
  availability_zone       = "${var.primary_region}a"
  map_public_ip_on_launch = true

  tags = {
    Name = "dr-lab-public-subnet"
  }
}

# Route Table
resource "aws_route_table" "public" {
  provider = aws.primary
  vpc_id   = aws_vpc.onprem.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.onprem.id
  }

  tags = {
    Name = "dr-lab-public-rt"
  }
}

resource "aws_route_table_association" "public" {
  provider       = aws.primary
  subnet_id      = aws_subnet.public.id
  route_table_id = aws_route_table.public.id
}

# Security Group - Allow SSH and outbound access
resource "aws_security_group" "onprem_server" {
  provider    = aws.primary
  name        = "dr-lab-onprem-sg"
  description = "Allow SSH and outbound traffic"
  vpc_id      = aws_vpc.onprem.id

  ingress {
    description = "SSH from anywhere"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    description = "Allow all outbound"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "dr-lab-onprem-sg"
  }
}

# IAM Role for EC2 to access S3
resource "aws_iam_role" "ec2_s3_backup" {
  provider = aws.primary
  name     = "dr-lab-ec2-s3-backup-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "ec2_s3_access" {
  provider = aws.primary
  name     = "dr-lab-s3-access"
  role     = aws_iam_role.ec2_s3_backup.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:PutObject",
          "s3:GetObject",
          "s3:ListBucket",
          "s3:DeleteObject"
        ]
        Resource = [
          aws_s3_bucket.primary.arn,
          "${aws_s3_bucket.primary.arn}/*"
        ]
      }
    ]
  })
}

resource "aws_iam_instance_profile" "ec2_backup" {
  provider = aws.primary
  name     = "dr-lab-ec2-backup-profile"
  role     = aws_iam_role.ec2_s3_backup.name
}

# Primary S3 Bucket (us-east-1)
resource "aws_s3_bucket" "primary" {
  provider      = aws.primary
  bucket        = "${var.bucket_prefix}-primary-${random_id.bucket_suffix.hex}"
  force_destroy = true

  tags = {
    Name        = "DR Lab Primary Bucket"
    Environment = "Production"
    Region      = var.primary_region
  }
}

resource "aws_s3_bucket_versioning" "primary" {
  provider = aws.primary
  bucket   = aws_s3_bucket.primary.id

  versioning_configuration {
    status = "Enabled"
  }
}

# Secondary S3 Bucket (us-west-2) - Replication Target
resource "aws_s3_bucket" "secondary" {
  provider      = aws.secondary
  bucket        = "${var.bucket_prefix}-secondary-${random_id.bucket_suffix.hex}"
  force_destroy = true

  tags = {
    Name        = "DR Lab Secondary Bucket"
    Environment = "DR"
    Region      = var.secondary_region
  }
}

resource "aws_s3_bucket_versioning" "secondary" {
  provider = aws.secondary
  bucket   = aws_s3_bucket.secondary.id

  versioning_configuration {
    status = "Enabled"
  }
}

# IAM Role for S3 Replication
resource "aws_iam_role" "replication" {
  provider = aws.primary
  name     = "dr-lab-s3-replication-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "s3.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "replication" {
  provider = aws.primary
  name     = "dr-lab-s3-replication-policy"
  role     = aws_iam_role.replication.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetReplicationConfiguration",
          "s3:ListBucket"
        ]
        Resource = aws_s3_bucket.primary.arn
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObjectVersionForReplication",
          "s3:GetObjectVersionAcl",
          "s3:GetObjectVersionTagging"
        ]
        Resource = "${aws_s3_bucket.primary.arn}/*"
      },
      {
        Effect = "Allow"
        Action = [
          "s3:ReplicateObject",
          "s3:ReplicateDelete",
          "s3:ReplicateTags"
        ]
        Resource = "${aws_s3_bucket.secondary.arn}/*"
      }
    ]
  })
}

# S3 Replication Configuration
resource "aws_s3_bucket_replication_configuration" "primary_to_secondary" {
  provider = aws.primary
  role     = aws_iam_role.replication.arn
  bucket   = aws_s3_bucket.primary.id

  rule {
    id     = "replicate-all"
    status = "Enabled"

    filter {}

    destination {
      bucket        = aws_s3_bucket.secondary.arn
      storage_class = "STANDARD"
    }
  }

  depends_on = [
    aws_s3_bucket_versioning.primary,
    aws_s3_bucket_versioning.secondary
  ]
}

# EC2 Instance - "On-Prem" Data Center Server
data "aws_ami" "amazon_linux" {
  provider    = aws.primary
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }
}

resource "aws_instance" "onprem_server" {
  provider               = aws.primary
  ami                    = data.aws_ami.amazon_linux.id
  instance_type          = "t3.micro"
  subnet_id              = aws_subnet.public.id
  vpc_security_group_ids = [aws_security_group.onprem_server.id]
  iam_instance_profile   = aws_iam_instance_profile.ec2_backup.name
  key_name               = var.key_pair_name

  root_block_device {
    volume_size = 20
    volume_type = "gp3"
  }

  tags = {
    Name        = "dr-lab-onprem-server"
    Environment = "Production"
    Purpose     = "Simulated On-Premises Data Center"
  }
}

# Random ID for unique bucket names
resource "random_id" "bucket_suffix" {
  byte_length = 4
}

# IAM Role for DataSync
resource "aws_iam_role" "datasync" {
  provider = aws.primary
  name     = "dr-lab-datasync-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "datasync.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "datasync_s3_access" {
  provider = aws.primary
  name     = "dr-lab-datasync-s3-policy"
  role     = aws_iam_role.datasync.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetBucketLocation",
          "s3:ListBucket",
          "s3:ListBucketMultipartUploads"
        ]
        Resource = aws_s3_bucket.primary.arn
      },
      {
        Effect = "Allow"
        Action = [
          "s3:AbortMultipartUpload",
          "s3:DeleteObject",
          "s3:GetObject",
          "s3:ListMultipartUploadParts",
          "s3:PutObject",
          "s3:GetObjectTagging",
          "s3:PutObjectTagging"
        ]
        Resource = "${aws_s3_bucket.primary.arn}/*"
      }
    ]
  })
}

# DataSync S3 Location
resource "aws_datasync_location_s3" "primary" {
  provider      = aws.primary
  s3_bucket_arn = aws_s3_bucket.primary.arn
  subdirectory  = "/business-data"

  s3_config {
    bucket_access_role_arn = aws_iam_role.datasync.arn
  }

  tags = {
    Name = "DR Lab S3 Destination"
  }
}
```

### Step 3: Create `variables.tf`

```hcl
variable "primary_region" {
  description = "Primary AWS region (simulated on-prem location)"
  type        = string
  default     = "us-east-1"
}

variable "secondary_region" {
  description = "Secondary AWS region (DR location)"
  type        = string
  default     = "us-west-2"
}

variable "bucket_prefix" {
  description = "Prefix for S3 bucket names"
  type        = string
  default     = "dr-lab-backup"
}

variable "key_pair_name" {
  description = "Name of existing EC2 key pair"
  type        = string
}
```

### Step 4: Create `outputs.tf`

```hcl
output "onprem_server_ip" {
  description = "Public IP of on-prem server"
  value       = aws_instance.onprem_server.public_ip
}

output "onprem_server_id" {
  description = "Instance ID of on-prem server"
  value       = aws_instance.onprem_server.id
}

output "primary_bucket_name" {
  description = "Name of primary S3 bucket"
  value       = aws_s3_bucket.primary.id
}

output "secondary_bucket_name" {
  description = "Name of secondary S3 bucket (DR)"
  value       = aws_s3_bucket.secondary.id
}

output "primary_bucket_region" {
  description = "Region of primary bucket"
  value       = var.primary_region
}

output "secondary_bucket_region" {
  description = "Region of secondary bucket"
  value       = var.secondary_region
}

output "ssh_command" {
  description = "SSH command to connect to on-prem server"
  value       = "ssh -i ~/.ssh/${var.key_pair_name}.pem ec2-user@${aws_instance.onprem_server.public_ip}"
}

output "datasync_s3_location_arn" {
  description = "DataSync S3 location ARN"
  value       = aws_datasync_location_s3.primary.arn
}
```

### Step 5: Create `terraform.tfvars`

```hcl
key_pair_name = "your-key-pair-name"  # Replace with your actual key pair name
bucket_prefix = "dr-lab-backup"
```

**Important:** Replace `your-key-pair-name` with the name of an existing EC2 key pair in your AWS account. If you don't have one, create it in the AWS Console first.

### Step 6: Deploy Infrastructure

```bash
terraform init
terraform plan
terraform apply
```

Save the outputs - you'll need them for the next steps:

```bash
terraform output -json > outputs.json
```

**Verification:**
- Check AWS Console: EC2 instance is running in us-east-1
- Two S3 buckets exist (us-east-1 and us-west-2)
- Replication is configured (check S3 bucket â†’ Management â†’ Replication)

---

## Part 2: Configure Automated Backup with Ansible

Now configure the "on-prem" server to automatically sync data to S3 using Ansible.

### Step 1: Create Ansible Inventory

```bash
# Extract the server IP from Terraform outputs
export SERVER_IP=$(terraform output -raw onprem_server_ip)

# Create inventory file
cat > inventory.ini <<EOF
[onprem_servers]
onprem ansible_host=${SERVER_IP} ansible_user=ec2-user ansible_ssh_private_key_file=~/.ssh/your-key-pair-name.pem

[onprem_servers:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no'
EOF
```

**Replace** `your-key-pair-name.pem` with your actual key file name.

### Step 2: Test Ansible Connection

```bash
ansible -i inventory.ini onprem_servers -m ping
```

Expected output:
```
onprem | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
```

### Step 3: Create Ansible Playbook - `configure_backup.yml`

```yaml
---
- name: Configure AWS DataSync for Automated Backup
  hosts: onprem_servers
  become: yes
  vars:
    backup_source_dir: /opt/business-data
    primary_bucket: "{{ lookup('pipe', 'terraform output -raw primary_bucket_name') }}"
    datasync_s3_location: "{{ lookup('pipe', 'terraform output -raw datasync_s3_location_arn') }}"
    aws_region: us-east-1

  tasks:
    - name: Install required packages
      yum:
        name:
          - aws-cli
          - nfs-utils
        state: present

    - name: Create business data directory
      file:
        path: "{{ backup_source_dir }}"
        state: directory
        mode: '0755'
        owner: ec2-user
        group: ec2-user

    - name: Create subdirectories for organized data
      file:
        path: "{{ backup_source_dir }}/{{ item }}"
        state: directory
        mode: '0755'
        owner: ec2-user
        group: ec2-user
      loop:
        - transactions
        - customer-records
        - reports
        - databases

    - name: Export NFS share for DataSync
      lineinfile:
        path: /etc/exports
        line: "{{ backup_source_dir }} *(rw,sync,no_root_squash,no_subtree_check)"
        create: yes

    - name: Start and enable NFS server
      systemd:
        name: nfs-server
        state: started
        enabled: yes

    - name: Export NFS shares
      command: exportfs -ra

    - name: Get instance metadata (private IP for NFS)
      uri:
        url: http://169.254.169.254/latest/meta-data/local-ipv4
        return_content: yes
      register: instance_private_ip

    - name: Create DataSync NFS source location
      command: >
        aws datasync create-location-nfs
        --server-hostname {{ instance_private_ip.content }}
        --subdirectory {{ backup_source_dir }}
        --on-prem-config AgentArns=[]
        --region {{ aws_region }}
        --output text
        --query 'LocationArn'
      register: datasync_nfs_location
      changed_when: true

    - name: Create DataSync task (NFS to S3)
      command: >
        aws datasync create-task
        --source-location-arn {{ datasync_nfs_location.stdout }}
        --destination-location-arn {{ datasync_s3_location }}
        --name "dr-lab-backup-task"
        --schedule ScheduleExpression="rate(1 minute)"
        --region {{ aws_region }}
        --output text
        --query 'TaskArn'
      register: datasync_task
      changed_when: true

    - name: Save DataSync task ARN to file
      copy:
        content: "{{ datasync_task.stdout }}"
        dest: /home/ec2-user/datasync_task_arn.txt
        owner: ec2-user
        group: ec2-user
        mode: '0644'

    - name: Display configuration summary
      debug:
        msg: |
          DataSync configuration complete!
          - Backup source: {{ backup_source_dir }}
          - NFS export configured
          - DataSync task: {{ datasync_task.stdout }}
          - Sync schedule: Every 1 minute
          - S3 destination: {{ primary_bucket }}/business-data/
          
          Upload files to {{ backup_source_dir }} and they'll automatically sync to S3.
```

### Step 4: Run the Ansible Playbook

```bash
ansible-playbook -i inventory.ini configure_backup.yml
```

**Expected output:**
```
PLAY RECAP *****************************************************
onprem : ok=11   changed=8   unreachable=0    failed=0    skipped=0
```

The playbook has configured everything - NFS, DataSync locations, and the scheduled task.

### Step 5: Create DR Recovery Playbook - `restore_from_dr.yml`

Create a second playbook for disaster recovery scenarios:

```yaml
---
- name: Disaster Recovery - Restore from S3 and Re-enable Backups
  hosts: onprem_servers
  become: yes
  vars:
    backup_source_dir: /opt/business-data
    primary_bucket: "{{ lookup('pipe', 'terraform output -raw primary_bucket_name') }}"
    datasync_s3_location: "{{ lookup('pipe', 'terraform output -raw datasync_s3_location_arn') }}"
    aws_region: us-east-1

  tasks:
    - name: Install required packages
      yum:
        name:
          - aws-cli
          - nfs-utils
        state: present

    - name: Create business data directory
      file:
        path: "{{ backup_source_dir }}"
        state: directory
        mode: '0755'
        owner: ec2-user
        group: ec2-user

    - name: Export NFS share for DataSync
      lineinfile:
        path: /etc/exports
        line: "{{ backup_source_dir }} *(rw,sync,no_root_squash,no_subtree_check)"
        create: yes

    - name: Start and enable NFS server
      systemd:
        name: nfs-server
        state: started
        enabled: yes

    - name: Export NFS shares
      command: exportfs -ra

    - name: Get instance metadata (private IP for NFS)
      uri:
        url: http://169.254.169.254/latest/meta-data/local-ipv4
        return_content: yes
      register: instance_private_ip

    - name: Create DataSync NFS location
      command: >
        aws datasync create-location-nfs
        --server-hostname {{ instance_private_ip.content }}
        --subdirectory {{ backup_source_dir }}
        --on-prem-config AgentArns=[]
        --region {{ aws_region }}
        --output text
        --query 'LocationArn'
      register: datasync_nfs_location
      changed_when: true

    - name: Create DataSync RESTORE task (S3 to NFS)
      command: >
        aws datasync create-task
        --source-location-arn {{ datasync_s3_location }}
        --destination-location-arn {{ datasync_nfs_location.stdout }}
        --name "dr-lab-restore-task"
        --region {{ aws_region }}
        --output text
        --query 'TaskArn'
      register: datasync_restore_task
      changed_when: true

    - name: Execute restore task
      command: >
        aws datasync start-task-execution
        --task-arn {{ datasync_restore_task.stdout }}
        --region {{ aws_region }}
        --output text
        --query 'TaskExecutionArn'
      register: restore_execution
      changed_when: true

    - name: Wait for data restoration to complete
      command: >
        aws datasync wait task-execution-complete
        --task-execution-arn {{ restore_execution.stdout }}
        --region {{ aws_region }}
      changed_when: false

    - name: Create DataSync BACKUP task (NFS to S3) with schedule
      command: >
        aws datasync create-task
        --source-location-arn {{ datasync_nfs_location.stdout }}
        --destination-location-arn {{ datasync_s3_location }}
        --name "dr-lab-backup-task"
        --schedule ScheduleExpression="rate(1 minute)"
        --region {{ aws_region }}
        --output text
        --query 'TaskArn'
      register: datasync_backup_task
      changed_when: true

    - name: Save DataSync backup task ARN to file
      copy:
        content: "{{ datasync_backup_task.stdout }}"
        dest: /home/ec2-user/datasync_task_arn.txt
        owner: ec2-user
        group: ec2-user
        mode: '0644'

    - name: Display recovery summary
      debug:
        msg: |
          Disaster Recovery Complete!
          - Data restored from S3 to {{ backup_source_dir }}
          - NFS export configured
          - Backup task re-enabled: {{ datasync_backup_task.stdout }}
          - Sync schedule: Every 1 minute
          
          Your server is now operational with all data restored.
```

This playbook:
1. Sets up NFS
2. Creates restore task (S3 â†’ NFS)
3. Executes restore and waits for completion
4. Creates backup task (NFS â†’ S3) with schedule
5. You're back online with automated backups re-enabled

---

## Part 3: Load Data and Verify Multi-Region Sync

### Step 1: Upload Files to "On-Prem" Server

Use SCP or SFTP to upload files to `/opt/business-data/` on the server.

### Step 2: Verify DataSync is Working

DataSync runs automatically every minute. After uploading files:

1. Wait ~60 seconds for DataSync to sync
2. Check the **S3 Console** - verify files appear in the primary bucket under `business-data/`
3. Check the **secondary bucket** (us-west-2) - verify cross-region replication worked

### Step 3: Monitor DataSync (Optional)

View DataSync execution history in the **AWS Console** under DataSync â†’ Tasks, or use CLI:

```bash
DATASYNC_TASK=$(cat ~/datasync_task_arn.txt)
aws datasync list-task-executions --task-arn ${DATASYNC_TASK} --max-results 5
```
- âœ… "On-prem" server with your business data
- âœ… AWS DataSync automated sync to S3 primary bucket (every minute)
- âœ… Cross-region replication to secondary bucket (us-west-2)
- âœ… Multi-region redundancy active
- âœ… Enterprise-grade backup solution

---

## Part 4: Disaster Recovery Drill - Simulate Data Center Failure

Time to test your DR plan and measure RTO/RPO.

### Step 1: Document Current State

SSH to the server and note how many files you have:

```bash
find /opt/business-data -type f | wc -l
```

Record this number - you'll verify it after recovery.

### Step 2: Simulate Catastrophic Failure

In the **AWS Console**, terminate the on-prem EC2 instance.

ðŸ’¥ **DATA CENTER DESTROYED - All local data is GONE**

### Step 3: Verify Data Survived in S3

Check the **S3 Console**:
- Primary bucket (us-east-1) should have all your data
- Secondary bucket (us-west-2) should have replicated copies

**RPO (Recovery Point Objective):** Maximum 1 minute (last DataSync sync)

### Step 4: Execute Disaster Recovery

Start timing your recovery:

```bash
# Start RTO timer
RTO_START=$(date +%s)

# Recreate infrastructure
terraform apply -auto-approve

# Wait for instance to be ready, then get new IP
export SERVER_IP=$(terraform output -raw onprem_server_ip)

# Update inventory with new IP
cat > inventory.ini <<EOF
[onprem_servers]
onprem ansible_host=${SERVER_IP} ansible_user=ec2-user ansible_ssh_private_key_file=~/.ssh/your-key-pair-name.pem

[onprem_servers:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no'
EOF

# Wait for SSH to be ready
sleep 30

# Run DR recovery playbook (restores data AND re-enables backups)
ansible-playbook -i inventory.ini restore_from_dr.yml
```

The `restore_from_dr.yml` playbook:
1. âœ… Sets up NFS
2. âœ… Creates DataSync restore task (S3 â†’ NFS)
3. âœ… Executes restore and waits for completion
4. âœ… Creates backup task (NFS â†’ S3) with schedule
5. âœ… You're back online!

### Step 5: Verify Recovery and Calculate RTO

```bash
# SSH to server and verify data
ssh -i ~/.ssh/your-key-pair-name.pem ec2-user@${SERVER_IP}
find /opt/business-data -type f | wc -l
# Should match the count from Step 1
exit

# Stop RTO timer
RTO_END=$(date +%s)
RTO_DURATION=$((RTO_END - RTO_START))

echo "ðŸŽ¯ Recovery Time Objective (RTO): ${RTO_DURATION} seconds ($((RTO_DURATION / 60)) minutes)"
echo "ðŸŽ¯ Recovery Point Objective (RPO): ~60 seconds (last DataSync sync)"
```

**Discussion:**
- How much data was lost? (None, if DataSync ran before termination)
- How long did recovery take? (Your measured RTO)
- What could speed up RTO? (Pre-built AMIs, parallel tasks)
- What could reduce RPO? (More frequent syncs, real-time replication)

**Checkpoint:**
- âœ… Survived complete data center destruction
- âœ… All data recovered from S3
- âœ… Infrastructure recreated via Terraform
- âœ… Configuration restored via Ansible
- âœ… Measured actual RTO/RPO

---

## Part 5: Test Geo-Resiliency and Region Failure Scenarios

Simulate even more extreme scenarios.

### Scenario 1: Primary Region Complete Outage

Imagine us-east-1 goes completely offline (has happened in real AWS outages).

**What happens?**

```bash
# Your data is STILL accessible in us-west-2
aws s3 ls s3://${SECONDARY_BUCKET}/business-data/ --recursive --region us-west-2

# Applications can be reconfigured to use secondary bucket
# Recovery server is already running in us-west-2
# No data loss (except in-flight sync data)
```

**Real-world example:** AWS us-east-1 outage in December 2021 affected many services for hours. Companies with multi-region architecture continued operating.

### Scenario 2: Ransomware Attack

Simulate malicious deletion:

```bash
# Attacker deletes a critical file on the "on-prem" server
# (if it was still running - but it's terminated in our lab)

# With S3 versioning enabled, you can recover:
aws s3api list-object-versions \
  --bucket ${SECONDARY_BUCKET} \
  --prefix business-data/database_backup.sql \
  --region us-west-2

# Shows all versions - can restore any previous version
```

### Key Resilience Concepts

**RTO (Recovery Time Objective):**
- Time between failure and resumed operations
- Our lab: ~5-10 minutes (provision + restore)
- Production: Can be seconds with active-active architecture

**RPO (Recovery Point Objective):**
- Maximum acceptable data loss
- Our lab: 60 seconds (sync interval)
- Production: Can be near-zero with real-time replication

**Disaster Recovery Tiers:**
```
Tier 1: Backup & Restore (hours RTO, hours RPO) - Cheapest
Tier 2: Pilot Light (10s of minutes RTO, minutes RPO)
Tier 3: Warm Standby (minutes RTO, seconds RPO)
Tier 4: Active-Active (seconds RTO, near-zero RPO) - Most expensive
```

Our lab implements **Tier 2-3** (between Pilot Light and Warm Standby).

### Best Practices Demonstrated

âœ… **Geographic redundancy** (multi-region)
âœ… **Automated backups** (no manual intervention)
âœ… **Versioning** (protect against accidental deletion)
âœ… **Infrastructure as Code** (reproducible DR environment)
âœ… **Configuration Management** (automated server setup)
âœ… **Regular testing** (you just did a DR drill!)

---

## Cleanup

```bash
terraform destroy -auto-approve
```

---

## Summary

Congratulations! You've built a complete disaster recovery solution using Terraform and Ansible. You learned:

1. **Multi-region infrastructure** with Terraform (primary + DR regions)
2. **Automated backup** with AWS DataSync (enterprise-grade sync to S3)
3. **Cross-region replication** for geo-redundancy
4. **DR testing** (simulated catastrophic failure and recovery)
5. **RTO/RPO measurement** (actual metrics from your environment)
6. **Hybrid cloud patterns** (simulated on-prem to cloud)
7. **Infrastructure as Code recovery** (terraform + ansible = operational)

### Real-World Applications

This architecture pattern is used by:
- **Financial services** (transaction data backup)
- **Healthcare** (patient records DR)
- **E-commerce** (order/inventory resilience)
- **SaaS companies** (customer data protection)

### Next Steps

**Enhancements to consider:**
- Add CloudWatch alarms for failed DataSync tasks
- Implement S3 Lifecycle policies (transition to Glacier)
- Add encryption at rest (S3 SSE-KMS)
- Create automated DR runbooks (AWS Systems Manager)
- Implement active-active architecture (Route 53 failover)
- Add compliance reporting (S3 Inventory, AWS Config)
- Use DataSync agents for real on-premises environments

### Additional Resources

- [AWS DataSync Documentation](https://docs.aws.amazon.com/datasync/)
- [AWS S3 Cross-Region Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html)
- [AWS Disaster Recovery Whitepaper](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-workloads-on-aws.html)
- [Terraform AWS Provider Documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Ansible Best Practices](https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html)

---

**Reflection Questions:**

1. What's the difference between backup and disaster recovery?
2. How would you design for a stricter RPO (e.g., <1 second)?
3. What are the trade-offs between RTO/RPO and cost?
4. How would this architecture change for a database (vs files)?
5. What additional failure scenarios should be tested?
