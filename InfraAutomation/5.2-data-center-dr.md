# Lab 5.2: Hybrid Cloud Disaster Recovery with Multi-Region Redundancy

## Overview

In this lab, you'll build a complete disaster recovery (DR) solution that simulates an on-premises data center with automated cloud backups and multi-region redundancy. You'll use Terraform to provision infrastructure, Ansible to configure servers, and AWS S3 cross-region replication to ensure data survives catastrophic failures.

**Real-world scenario:** Companies need to protect critical business data against both server failures and regional disasters. You'll implement a hybrid cloud backup strategy where an "on-premises" server continuously syncs data to AWS S3, with automatic replication to a secondary geographic region.

**Technical approach:** You'll create a systemd timer that acts as a lightweight backup agent, running `aws s3 sync` every minute to automatically replicate data to S3. This combines familiar tools (systemd from Linux week, AWS CLI from this week) to create an enterprise-grade backup solution with minimal complexity.

### What You'll Learn

- Infrastructure as Code with Terraform (multi-region architecture)
- Configuration Management with Ansible (automated backup setup)
- systemd timers as automated backup agents
- AWS S3 cross-region replication for geo-redundancy
- Disaster recovery procedures (RTO/RPO planning)
- Simulating and recovering from catastrophic failures
- Hybrid cloud backup strategies

### Key Disaster Recovery Concepts

**RPO (Recovery Point Objective):** How much data you can afford to lose, measured in time. If your RPO is 1 minute, you can tolerate losing up to 1 minute of data in a disaster. This determines how often you need to back up.

**RTO (Recovery Time Objective):** How long it takes to get your systems back online after a disaster. If your RTO is 10 minutes, your business can tolerate 10 minutes of downtime. This determines how fast your recovery process needs to be.

**Example:** An e-commerce site with 1-minute RPO and 10-minute RTO means they backup every minute and can restore operations within 10 minutes of a failure.

### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                         PRIMARY REGION (us-east-1)              │
│  ┌──────────────────┐          ┌─────────────────────────┐     │
│  │  EC2 "On-Prem"   │  Sync    │   S3 Primary Bucket     │     │
│  │  Data Center     ├─────────►│   (Business Data)       │     │
│  │  Server          │          │                         │     │
│  └──────────────────┘          └──────────┬──────────────┘     │
│                                            │                     │
└────────────────────────────────────────────┼─────────────────────┘
                                             │
                                             │ Cross-Region
                                             │ Replication
                                             ▼
┌─────────────────────────────────────────────────────────────────┐
│                      SECONDARY REGION (us-west-2)               │
│                    ┌─────────────────────────┐                  │
│                    │   S3 Replica Bucket     │                  │
│                    │   (DR Copy)             │                  │
│                    │                         │                  │
│                    └─────────────────────────┘                  │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```

### Time Estimate
90-120 minutes

### Prerequisites

- AWS CLI configured with credentials
- Terraform installed (v1.0+)
- Ansible installed (v2.9+)
- SSH key pair for EC2 access
- Basic understanding of DR concepts

---

## Part 1: Provision Multi-Region Infrastructure with Terraform

Create the cloud infrastructure that will support your hybrid backup solution.

### Step 1: Create Terraform Configuration Directory

```bash
mkdir ~/dr-lab
cd ~/dr-lab
```

### Step 2: Create `main.tf` - Primary Infrastructure

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Primary region provider (us-east-1)
provider "aws" {
  region = var.primary_region
  alias  = "primary"
}

# Secondary region provider (us-west-2)
provider "aws" {
  region = var.secondary_region
  alias  = "secondary"
}

# VPC for "on-prem" server
resource "aws_vpc" "onprem" {
  provider             = aws.primary
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = "dr-lab-onprem-vpc"
  }
}

# Internet Gateway
resource "aws_internet_gateway" "onprem" {
  provider = aws.primary
  vpc_id   = aws_vpc.onprem.id

  tags = {
    Name = "dr-lab-igw"
  }
}

# Public Subnet
resource "aws_subnet" "public" {
  provider                = aws.primary
  vpc_id                  = aws_vpc.onprem.id
  cidr_block              = "10.0.1.0/24"
  availability_zone       = "${var.primary_region}a"
  map_public_ip_on_launch = true

  tags = {
    Name = "dr-lab-public-subnet"
  }
}

# Route Table
resource "aws_route_table" "public" {
  provider = aws.primary
  vpc_id   = aws_vpc.onprem.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.onprem.id
  }

  tags = {
    Name = "dr-lab-public-rt"
  }
}

resource "aws_route_table_association" "public" {
  provider       = aws.primary
  subnet_id      = aws_subnet.public.id
  route_table_id = aws_route_table.public.id
}

# Security Group - Allow SSH and outbound access
resource "aws_security_group" "onprem_server" {
  provider    = aws.primary
  name        = "dr-lab-onprem-sg"
  description = "Allow SSH and outbound traffic"
  vpc_id      = aws_vpc.onprem.id

  ingress {
    description = "SSH from anywhere"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    description = "Allow all outbound"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "dr-lab-onprem-sg"
  }
}

# IAM Role for EC2 to access S3
resource "aws_iam_role" "ec2_s3_backup" {
  provider = aws.primary
  name     = "dr-lab-ec2-s3-backup-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "ec2_s3_access" {
  provider = aws.primary
  name     = "dr-lab-s3-access"
  role     = aws_iam_role.ec2_s3_backup.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:PutObject",
          "s3:GetObject",
          "s3:ListBucket",
          "s3:DeleteObject"
        ]
        Resource = [
          aws_s3_bucket.primary.arn,
          "${aws_s3_bucket.primary.arn}/*"
        ]
      }
    ]
  })
}

resource "aws_iam_instance_profile" "ec2_backup" {
  provider = aws.primary
  name     = "dr-lab-ec2-backup-profile"
  role     = aws_iam_role.ec2_s3_backup.name
}

# Primary S3 Bucket (us-east-1)
resource "aws_s3_bucket" "primary" {
  provider      = aws.primary
  bucket        = "${var.bucket_prefix}-primary-${random_id.bucket_suffix.hex}"
  force_destroy = true

  tags = {
    Name        = "DR Lab Primary Bucket"
    Environment = "Production"
    Region      = var.primary_region
  }
}

resource "aws_s3_bucket_versioning" "primary" {
  provider = aws.primary
  bucket   = aws_s3_bucket.primary.id

  versioning_configuration {
    status = "Enabled"
  }
}

# Secondary S3 Bucket (us-west-2) - Replication Target
resource "aws_s3_bucket" "secondary" {
  provider      = aws.secondary
  bucket        = "${var.bucket_prefix}-secondary-${random_id.bucket_suffix.hex}"
  force_destroy = true

  tags = {
    Name        = "DR Lab Secondary Bucket"
    Environment = "DR"
    Region      = var.secondary_region
  }
}

resource "aws_s3_bucket_versioning" "secondary" {
  provider = aws.secondary
  bucket   = aws_s3_bucket.secondary.id

  versioning_configuration {
    status = "Enabled"
  }
}

# IAM Role for S3 Replication
resource "aws_iam_role" "replication" {
  provider = aws.primary
  name     = "dr-lab-s3-replication-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "s3.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "replication" {
  provider = aws.primary
  name     = "dr-lab-s3-replication-policy"
  role     = aws_iam_role.replication.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetReplicationConfiguration",
          "s3:ListBucket"
        ]
        Resource = aws_s3_bucket.primary.arn
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObjectVersionForReplication",
          "s3:GetObjectVersionAcl",
          "s3:GetObjectVersionTagging"
        ]
        Resource = "${aws_s3_bucket.primary.arn}/*"
      },
      {
        Effect = "Allow"
        Action = [
          "s3:ReplicateObject",
          "s3:ReplicateDelete",
          "s3:ReplicateTags"
        ]
        Resource = "${aws_s3_bucket.secondary.arn}/*"
      }
    ]
  })
}

# S3 Replication Configuration
resource "aws_s3_bucket_replication_configuration" "primary_to_secondary" {
  provider = aws.primary
  role     = aws_iam_role.replication.arn
  bucket   = aws_s3_bucket.primary.id

  rule {
    id     = "replicate-all"
    status = "Enabled"

    filter {}

    delete_marker_replication {
      status = "Enabled"
    }

    destination {
      bucket        = aws_s3_bucket.secondary.arn
      storage_class = "STANDARD"
    }
  }

  depends_on = [
    aws_s3_bucket_versioning.primary,
    aws_s3_bucket_versioning.secondary
  ]
}

# EC2 Instance - "On-Prem" Data Center Server
data "aws_ami" "amazon_linux" {
  provider    = aws.primary
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }
}

resource "aws_instance" "onprem_server" {
  provider               = aws.primary
  ami                    = data.aws_ami.amazon_linux.id
  instance_type          = "t3.micro"
  subnet_id              = aws_subnet.public.id
  vpc_security_group_ids = [aws_security_group.onprem_server.id]
  iam_instance_profile   = aws_iam_instance_profile.ec2_backup.name
  key_name               = var.key_pair_name

  root_block_device {
    volume_size = 20
    volume_type = "gp3"
  }

  tags = {
    Name        = "dr-lab-onprem-server"
    Environment = "Production"
    Purpose     = "Simulated On-Premises Data Center"
  }
}

# Random ID for unique bucket names
resource "random_id" "bucket_suffix" {
  byte_length = 4
}
```

### Step 3: Create `variables.tf`

```hcl
variable "primary_region" {
  description = "Primary AWS region (simulated on-prem location)"
  type        = string
  default     = "us-east-1"
}

variable "secondary_region" {
  description = "Secondary AWS region (DR location)"
  type        = string
  default     = "us-west-2"
}

variable "bucket_prefix" {
  description = "Prefix for S3 bucket names"
  type        = string
  default     = "dr-lab-backup"
}

variable "key_pair_name" {
  description = "Name of existing EC2 key pair"
  type        = string
}
```

### Step 4: Create `outputs.tf`

```hcl
output "onprem_server_ip" {
  description = "Public IP of on-prem server"
  value       = aws_instance.onprem_server.public_ip
}

output "onprem_server_id" {
  description = "Instance ID of on-prem server"
  value       = aws_instance.onprem_server.id
}

output "primary_bucket_name" {
  description = "Name of primary S3 bucket"
  value       = aws_s3_bucket.primary.id
}

output "secondary_bucket_name" {
  description = "Name of secondary S3 bucket (DR)"
  value       = aws_s3_bucket.secondary.id
}

output "primary_bucket_region" {
  description = "Region of primary bucket"
  value       = var.primary_region
}

output "secondary_bucket_region" {
  description = "Region of secondary bucket"
  value       = var.secondary_region
}
```

### Step 5: Create `terraform.tfvars`

```hcl
key_pair_name = "your-key-pair-name"  # Replace with your actual key pair name
bucket_prefix = "dr-lab-backup"
```

**Important:** Replace `your-key-pair-name` with the name of an existing EC2 key pair in your AWS account. If you don't have one, create it in the AWS Console first.

### Step 6: Deploy Infrastructure

```bash
terraform init
terraform plan
terraform apply
```

Save the outputs - you'll need them for the next steps:

```bash
terraform output -json > outputs.json
```

**Verification:**
- Check AWS Console: EC2 instance is running in us-east-1
- Two S3 buckets exist (us-east-1 and us-west-2)
- Replication is configured (check S3 bucket → Management → Replication)

---

## Part 2: Configure Automated Backup with Ansible

Now configure the "on-prem" server to automatically sync data to S3 using Ansible.

**Note:** Run these commands from a machine with Ansible installed (AWS Cloud Shell, your Cloud9 instance, or the instance you used for previous Ansible labs).



### Step 1: Create Ansible Inventory

Get your server IP from Terraform outputs, then create the inventory file:

```bash
cat > inventory.ini <<EOF
[onprem_servers]
onprem ansible_host=YOUR_SERVER_IP ansible_user=ec2-user ansible_ssh_private_key_file=~/.ssh/your-key-pair-name.pem

[onprem_servers:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no'
EOF
```

**Replace:**
- `YOUR_SERVER_IP` with the public IP from `terraform output onprem_server_ip`
- `your-key-pair-name.pem` with your actual key file name

### Step 2: Test Ansible Connection

```bash
ansible -i inventory.ini onprem_servers -m ping
```

Expected output:
```
onprem | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
```

### Step 3: Create Ansible Playbook - `configure_backup.yml`

```yaml
---
- name: Configure S3 Sync for Automated Backup and Recovery
  hosts: onprem_servers
  become: yes
  vars:
    backup_source_dir: /opt/business-data
    primary_bucket: "YOUR_PRIMARY_BUCKET_NAME"  # Replace with bucket name from terraform output
    aws_region: us-east-1

  tasks:
    - name: Install required packages
      yum:
        name:
          - aws-cli
        state: present

    - name: Create business data directory
      file:
        path: "{{ backup_source_dir }}"
        state: directory
        mode: '0755'
        owner: ec2-user
        group: ec2-user

    - name: Check if local directory has files
      find:
        paths: "{{ backup_source_dir }}"
        file_type: file
      register: local_files

    - name: Restore from S3 if local directory is empty
      command: >
        aws s3 sync s3://{{ primary_bucket }}/business-data/ {{ backup_source_dir }}
      when: local_files.matched == 0
      changed_when: true

    - name: Set ownership on restored files
      file:
        path: "{{ backup_source_dir }}"
        owner: ec2-user
        group: ec2-user
        recurse: yes
      when: local_files.matched == 0

    - name: Create systemd service for S3 sync
      copy:
        dest: /etc/systemd/system/s3-backup.service
        content: |
          [Unit]
          Description=Sync business data to S3
          After=network.target

          [Service]
          Type=oneshot
          ExecStart=/usr/bin/aws s3 sync {{ backup_source_dir }} s3://{{ primary_bucket }}/business-data/
          User=root

          [Install]
          WantedBy=multi-user.target
        mode: '0644'

    - name: Create systemd timer for S3 sync
      copy:
        dest: /etc/systemd/system/s3-backup.timer
        content: |
          [Unit]
          Description=Run S3 backup every minute
          
          [Timer]
          OnBootSec=1min
          OnUnitActiveSec=1min
          
          [Install]
          WantedBy=timers.target
        mode: '0644'

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start S3 backup timer
      systemd:
        name: s3-backup.timer
        state: started
        enabled: yes

    - name: Display configuration summary
      debug:
        msg: |
          S3 sync configuration complete!
          - Backup source: {{ backup_source_dir }}
          - S3 destination: s3://{{ primary_bucket }}/business-data/
          - Sync schedule: Every 1 minute (systemd timer)
          - Files restored from S3: {{ local_files.matched == 0 }}
          
          The system will automatically restore from S3 if the directory is empty.
          Check sync status: systemctl status s3-backup.timer
```

### Step 4: Edit Playbook Variables

Before running the playbook, edit `configure_backup.yml` and replace `YOUR_PRIMARY_BUCKET_NAME` with the actual bucket name:

```bash
# Get bucket name from Terraform
terraform output primary_bucket_name

# Edit the playbook and replace YOUR_PRIMARY_BUCKET_NAME with the output above
```

### Step 5: Run the Ansible Playbook

```bash
ansible-playbook -i inventory.ini configure_backup.yml
```

**Expected output:**
```
PLAY RECAP *****************************************************
onprem : ok=10   changed=8   unreachable=0    failed=0    skipped=0
```

The playbook automatically detects if the directory is empty and restores from S3 if needed, then sets up continuous backups.

---

## Part 3: Load Data and Verify Multi-Region Sync

### Step 1: Upload Files to "On-Prem" Server

Use SCP or SFTP to upload files to `/opt/business-data/` on the server.

### Step 2: Verify S3 Sync is Working

The systemd timer runs every minute. After uploading files:

1. Wait ~60 seconds for the sync to run
2. Check the **S3 Console** - verify files appear in the primary bucket under `business-data/`
3. Check the **secondary bucket** (us-west-2) - verify cross-region replication worked

### Step 3: Monitor Sync Status (Optional)

SSH to the server and check the timer status:

```bash
sudo systemctl status s3-backup.timer
sudo journalctl -u s3-backup.service -n 20
```

**Checkpoint:**
- "On-prem" server with your business data
- Automated S3 sync every minute to primary bucket
- Cross-region replication to secondary bucket (us-west-2)
- Multi-region redundancy active
- Enterprise-grade backup solution

---

## Part 4: Disaster Recovery Drill - Simulate Data Center Failure

Time to test your DR plan and measure RTO/RPO.

### Step 1: Document Current State

SSH to the server and note how many files you have:

```bash
find /opt/business-data -type f | wc -l
```

Record this number - you'll verify it after recovery.

### Step 2: Simulate Catastrophic Failure

In the **AWS Console**, terminate the on-prem EC2 instance.

**DATA CENTER DESTROYED - All local data is GONE**

### Step 3: Verify Data Survived in S3

Check the **S3 Console**:
- Primary bucket (us-east-1) should have all your data
- Secondary bucket (us-west-2) should have replicated copies

**RPO (Recovery Point Objective):** Maximum 1 minute (last S3 sync). 

### Step 4: Execute Disaster Recovery

Recreate infrastructure and restore data:

```bash
# Recreate infrastructure
terraform apply -auto-approve

# Get new IP and update inventory.ini
terraform output onprem_server_ip
# Replace YOUR_SERVER_IP in inventory.ini with the IP above

# Wait ~30 seconds for instance to boot, then run the playbook
ansible-playbook -i inventory.ini configure_backup.yml
```

The playbook will automatically:
1. Detect the directory is empty
2. Restore all data from S3
3. Set up systemd timer for continuous backups
4. You're back online!

### Step 5: Verify Recovery and Calculate RTO

SSH to the server and verify data:

```bash
find /opt/business-data -type f | wc -l
# Should match the count from Step 1
```

**RTO/RPO Summary:**
- Recovery Time Objective (RTO): ~5-10 minutes (infrastructure + restore)
- Recovery Point Objective (RPO): ~60 seconds (last S3 sync)

**Discussion:**
- How much data was lost? (None, if DataSync ran before termination)
- How long did recovery take? (Your measured RTO)
- What could speed up RTO? (Pre-built AMIs, parallel tasks)
- What could reduce RPO? (More frequent syncs, real-time replication)

**Checkpoint:**
- Survived complete data center destruction
- All data recovered from S3
- Infrastructure recreated via Terraform
- Configuration restored via Ansible
- Measured actual RTO/RPO

---

### Key Resilience Concepts

**RTO (Recovery Time Objective):**
- Time between failure and resumed operations
- Our lab: ~5-10 minutes (provision + restore)
- Production: Can be seconds with active-active architecture

**RPO (Recovery Point Objective):**
- Maximum acceptable data loss
- Our lab: 60 seconds (sync interval)
- Production: Can be near-zero with real-time replication

**Disaster Recovery Tiers:**
```
Tier 1: Backup & Restore (hours RTO, hours RPO) - Cheapest
Tier 2: Pilot Light (10s of minutes RTO, minutes RPO)
Tier 3: Warm Standby (minutes RTO, seconds RPO)
Tier 4: Active-Active (seconds RTO, near-zero RPO) - Most expensive
```

Our lab implements **Tier 1-2** (Backup & Restore with automation, approaching Pilot Light).

### Best Practices Demonstrated

- **Geographic redundancy** (multi-region)
- **Automated backups** (no manual intervention)
- **Versioning** (protect against accidental deletion)
- **Infrastructure as Code** (reproducible DR environment)
- **Configuration Management** (automated server setup)
- **Regular testing** (you just did a DR drill!)

---

## Cleanup

```bash
terraform destroy -auto-approve
```

---

## Summary

Congratulations! You've built a complete disaster recovery solution using Terraform and Ansible. You learned:

1. **Multi-region infrastructure** with Terraform (primary + DR regions)
2. **Automated backup** with systemd timers and `aws s3 sync`
3. **Cross-region replication** for geo-redundancy
4. **DR testing** (simulated catastrophic failure and recovery)
5. **RTO/RPO measurement** (actual metrics from your environment)
6. **Hybrid cloud patterns** (simulated on-prem to cloud)
7. **Infrastructure as Code recovery** (terraform + ansible = operational)

### Real-World Applications

This architecture pattern is used by:
- **Financial services** (transaction data backup)
- **Healthcare** (patient records DR)
- **E-commerce** (order/inventory resilience)
- **SaaS companies** (customer data protection)

### Next Steps

**Enhancements to consider:**
- Use AWS DataSync agents for production environments (more robust, handles network interruptions)
- Add CloudWatch alarms for failed S3 sync operations (check journalctl logs)
- Implement S3 Lifecycle policies (transition to Glacier)
- Add encryption at rest (S3 SSE-KMS)
- Create automated DR runbooks (AWS Systems Manager)
- Implement active-active architecture (Route 53 failover)
- Add compliance reporting (S3 Inventory, AWS Config)
- Use more frequent sync intervals or event-driven syncs

### Additional Resources

- [AWS S3 CLI Documentation](https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html)
- [AWS S3 Cross-Region Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html)
- [AWS Disaster Recovery Whitepaper](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-workloads-on-aws.html)
- [Terraform AWS Provider Documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Ansible Best Practices](https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html)
- [systemd Timers](https://www.freedesktop.org/software/systemd/man/systemd.timer.html)

---

**Reflection Questions:**

1. What's the difference between backup and disaster recovery?
2. How would you design for a stricter RPO (e.g., <1 second)?
3. What are the trade-offs between RTO/RPO and cost?
4. How would this architecture change for a database (vs files)?
5. What additional failure scenarios should be tested?
